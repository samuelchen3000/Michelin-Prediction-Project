{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vFvXXPAZDHAn"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# reviews = pd.read_csv(\"austin_yelp_reviews.csv\")\n",
        "# unique = reviews['Restaurant Name'].unique()\n",
        "# print(unique)\n",
        "# pd.DataFrame(unique).to_csv('austin_restaurants.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "austin_reviews = pd.read_csv(\"austin_yelp_reviews.csv\")\n",
        "michelin_reviews = pd.read_csv(\"Michelin Reviews.csv\")\n",
        "non_michelin_reviews = pd.read_csv(\"yelp_reviews_non_michelin.csv\")"
      ],
      "metadata": {
        "id": "MPv2j3L8NHdu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "michelin_reviews['Review Length'] = michelin_reviews['Review'].apply(lambda x: len(x.split()))\n",
        "non_michelin_reviews['Review Length'] = non_michelin_reviews['Review'].apply(lambda x: len(x.split()))\n",
        "print(michelin_reviews['Review Length'].mean())\n",
        "print(non_michelin_reviews['Review Length'].mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVOEpiSlOsdx",
        "outputId": "b1450b15-74bc-4332-f1cf-48a93e5e6b6f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "169.21166343632424\n",
            "122.57221761944214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "austin_reviews['Review Length'] = austin_reviews['Review'].apply(lambda x: len(x.split()))\n",
        "avg_review_length_per_restaurant = austin_reviews.groupby('Restaurant Name')['Review Length'].mean()\n",
        "print(avg_review_length_per_restaurant.sort_values(ascending=False).head(50))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6d5oaFyO7Pf",
        "outputId": "76305050-9cca-409f-946b-15076d00c7cb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Restaurant Name\n",
            "The Driskill Grill                  199.020000\n",
            "Conje                               176.200000\n",
            "Franklin Barbecue                   175.770000\n",
            "dipdipdip tatsuya                   175.030000\n",
            "Arlo Grey                           172.850000\n",
            "Sushi Endo                          168.750000\n",
            "Otoko                               168.710000\n",
            "Juniper                             162.920000\n",
            "Tsuke Edomae                        158.289157\n",
            "Garrison                            157.220000\n",
            "The Guest House                     157.190000\n",
            "Uchiko                              155.950000\n",
            "Uchi                                155.950000\n",
            "Lenoir                              155.410000\n",
            "Hestia                              153.500000\n",
            "Intero                              152.580000\n",
            "Kemuri Tatsu-Ya                     152.376238\n",
            "Patika Coffee                       152.250000\n",
            "Craft Omakase                       151.735294\n",
            "Contigo                             148.363636\n",
            "Honey Moon Spirit Lounge            146.430000\n",
            "LÕOca dÕOro                         146.420000\n",
            "Sway                                145.620000\n",
            "Blackâs BBQ                       145.380000\n",
            "Terry Blackâ's BBQ                  145.380000\n",
            "Emmer & Rye                         145.150000\n",
            "Aba                                 144.590000\n",
            "Barley Swine                        144.290000\n",
            "Comedor                             144.150000\n",
            "Salty Sow                           142.495050\n",
            "Prelude                             142.066667\n",
            "Carpenter Hotel                     141.827160\n",
            "Birdies                             140.100000\n",
            "JeffreyÕs                           140.020000\n",
            "Soto South Lamar                    139.910000\n",
            "Olamaie                             137.630000\n",
            "Pasta|Bar                           137.032787\n",
            "Lutie's                             136.860000\n",
            "Vixen's Wedding                     136.060000\n",
            "Paperboy                            135.640000\n",
            "Lin Asian Bar + Dim Sum             135.590000\n",
            "Vespaio                             135.370000\n",
            "Nixta Taqueria                      133.860000\n",
            "Odd Duck                            133.490000\n",
            "Odd Duck Truck                      133.490000\n",
            "Red Farm                            133.450000\n",
            "PerryÕs Steakhouse & Grille         133.280000\n",
            "Suerte Eastside                     133.140000\n",
            "Suerte                              133.140000\n",
            "Jacoby's Restaurant & Mercantile    131.780000\n",
            "Name: Review Length, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# review length, sentiment score, michelin principles\n",
        "# use z-score"
      ],
      "metadata": {
        "id": "AAVuDsgWPvHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "#file_path = 'PUT YOUR FILE HERE  # Replace with the actual file path\n",
        "reviews_df = pd.read_csv('yelp_reviews_non_michelin.csv')\n",
        "# Rename columns if necessary\n",
        "reviews_df.columns = ['Restaurant Name', 'Location', 'Review']\n",
        "# Load spaCy's language model for Named Entity Recognition\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "# Function to extract various relevant entities\n",
        "def extract_relevant_entities(text):\n",
        "    doc = nlp(text)\n",
        "    # Define the entities we are interested in\n",
        "    relevant_entities = {\n",
        "        'PERSON': [],   # Chefs, Staff, etc.\n",
        "        'FOOD': [],     # Food-related entities\n",
        "        'ORG': [],      # Organizations\n",
        "        'PRODUCT': [],  # Food-related products\n",
        "        'FAC': []      # Facilites\n",
        "    }\n",
        "    # Extract entities and categorize them\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in relevant_entities.keys():\n",
        "            relevant_entities[ent.label_].append(ent.text)\n",
        "    return relevant_entities\n",
        "# Apply the function to extract relevant entities from reviews\n",
        "reviews_df['entities'] = reviews_df['Review'].apply(extract_relevant_entities)\n",
        "# Display first few rows to see extracted entities\n",
        "print(reviews_df[['Restaurant Name', 'entities']].head())\n",
        "# Optional: remove reviews with no entities (if you want to filter by entity presence)\n",
        "reviews_with_entities = reviews_df[reviews_df['entities'].apply(lambda x: any(x.values()))]\n",
        "# Topic Modeling based on relevant entities\n",
        "def perform_topic_modeling(corpus, num_topics=5, num_words=10):\n",
        "    vectorizer = CountVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
        "    lda.fit(X)\n",
        "    # Display topics and top words\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    for topic_idx, topic in enumerate(lda.components_):\n",
        "        top_features = [feature_names[i] for i in topic.argsort()[:-num_words - 1:-1]]\n",
        "        print(f\"Topic {topic_idx + 1}: {', '.join(top_features)}\")\n",
        "# Create a new column by joining extracted entities to form a corpus for topic modeling\n",
        "reviews_df['entity_corpus'] = reviews_df['entities'].apply(lambda x: ' '.join([item for sublist in x.values() for item in sublist]))\n",
        "# Filter for reviews with at least some entities\n",
        "filtered_reviews = reviews_df[reviews_df['entity_corpus'] != '']\n",
        "# Perform topic modeling on extracted entities\n",
        "perform_topic_modeling(filtered_reviews['entity_corpus'], num_topics=5, num_words=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdsCNYOhbKVE",
        "outputId": "8b26e790-4d3e-4731-db54-f4d1a10287be"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Restaurant Name                                           entities\n",
            "0          Lucali  {'PERSON': [], 'FOOD': [], 'ORG': [], 'PRODUCT...\n",
            "1          Lucali  {'PERSON': [], 'FOOD': [], 'ORG': [], 'PRODUCT...\n",
            "2          Lucali  {'PERSON': ['David Beckham'], 'FOOD': [], 'ORG...\n",
            "3          Lucali  {'PERSON': [], 'FOOD': [], 'ORG': [], 'PRODUCT...\n",
            "4          Lucali  {'PERSON': [], 'FOOD': [], 'ORG': ['NY'], 'PRO...\n",
            "Topic 1: benedict, covid, goody, patty, la, wow, arancini, thai, works, cafe\n",
            "Topic 2: michelin, mac, bbq, chef, cheese, tacos, mary, burger, bartaco, pepe\n",
            "Topic 3: al, el, food, steak, ny, dente, michelin, sushi, bourbon, truffle\n",
            "Topic 4: taco, dessert, joe, al, michelin, miami, spice, valet, tacos, lobster\n",
            "Topic 5: chicken, crispy, fried, strawberry, shrimp, rice, salad, dan, brussel, yelp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lda\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZDiIUiegtWs",
        "outputId": "e249401d-6fce-4a33-c40e-b217a3d71a17"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lda\n",
            "  Downloading lda-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from lda) (1.26.4)\n",
            "Downloading lda-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (350 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m350.0/350.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lda\n",
            "Successfully installed lda-3.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/python3.6\n",
        "#Install LDA library if not already installed\n",
        "# pip3.6 install --user lda\n",
        "# the input file is yelp_reviews.xslx\n",
        "# there are two output files: topic_word_dist.xlsx and document_topic_dist.xlsx\n",
        "# the script prompts for the name of the columns -- Restaurant_name and Restaurant_review in the yelp_reviews.xslx file\n",
        "import os, csv, nltk, lda\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import PunktSentenceTokenizer, RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import PunktSentenceTokenizer,RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "reviews_df=pd.read_csv(\"Michelin Reviews.csv\",encoding='utf8')\n",
        "#checking for nulls if present any\n",
        "print(\"Number of rows with any of the empty columns:\")\n",
        "print(reviews_df.isnull().sum().sum())\n",
        "\n",
        "reviews_df=reviews_df.dropna()\n",
        "restaurant_name = input('provide the column name for id: ')\n",
        "restaurant_review = input('provide the column name for text: ')\n",
        "ntopics= input('Provide the number of latent topics: ')\n",
        "\n",
        "word_tokenizer=RegexpTokenizer(r'\\w+')\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "stopwords_nltk=set(stopwords.words('english'))\n",
        "\n",
        "def tokenize_text(version_desc):\n",
        "  lowercase=version_desc.lower()\n",
        "  text = wordnet_lemmatizer.lemmatize(lowercase)\n",
        "  tokens = word_tokenizer.tokenize(text)\n",
        "  return tokens\n",
        "\n",
        "vec_words = CountVectorizer(tokenizer=tokenize_text,stop_words=list(stopwords_nltk),decode_error='ignore')\n",
        "total_features_words = vec_words.fit_transform(reviews_df[restaurant_review])\n",
        "print(total_features_words.shape)\n",
        "\n",
        "model = lda.LDA(n_topics=int(ntopics), n_iter=500, random_state=1)\n",
        "model.fit(total_features_words)\n",
        "topic_word = model.topic_word_\n",
        "doc_topic=model.doc_topic_\n",
        "doc_topic=pd.DataFrame(doc_topic)\n",
        "reviews_df=reviews_df.join(doc_topic)\n",
        "restaurant=pd.DataFrame()\n",
        "\n",
        "for i in range(int(ntopics)):\n",
        "  topic=\"topic_\"+str(i)\n",
        "  restaurant[topic]=reviews_df.groupby([restaurant_name])[i].mean()\n",
        "\n",
        "restaurant=restaurant.reset_index()\n",
        "topics=pd.DataFrame(topic_word)\n",
        "topics.columns=vec_words.get_feature_names_out()\n",
        "topics1=topics.transpose()\n",
        "print (\"Topics word distribution written in file topic_word_dist.xlsx \")\n",
        "topics1.to_excel(\"topic_word_dist.xlsx\")\n",
        "restaurant.to_excel(\"document_topic_dist.xlsx\",index=False)\n",
        "print (\"Document topic distribution written in file document_topic_dist.xlsx \")\n",
        "print(restaurant)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV98pbpobpla",
        "outputId": "cb7a3ed6-0847-4404-bf0a-5c30271e34be"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of rows with any of the empty columns:\n",
            "0\n",
            "provide the column name for id: Restaurant Name\n",
            "provide the column name for text: Review\n",
            "Provide the number of latent topics: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'ha', 'wa'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17542, 42735)\n",
            "Topics word distribution written in file topic_word_dist.xlsx \n",
            "Document topic distribution written in file document_topic_dist.xlsx \n",
            "         Restaurant Name   topic_0   topic_1   topic_2\n",
            "0                7 Adams  0.659648  0.039831  0.300521\n",
            "1             Acquerello  0.689485  0.041233  0.269283\n",
            "2                Addison  0.771234  0.139097  0.089670\n",
            "3                   Albi  0.686030  0.024457  0.289513\n",
            "4                 Alinea  0.840562  0.069460  0.089978\n",
            "..                   ...       ...       ...       ...\n",
            "191  Victoria & Albert's  0.835096  0.143065  0.021839\n",
            "192             Wakuriya  0.714106  0.169360  0.116535\n",
            "193              Yoshino  0.590203  0.362281  0.047516\n",
            "194               n/naka  0.680171  0.231698  0.088131\n",
            "195                  odo  0.641789  0.207953  0.150258\n",
            "\n",
            "[196 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2wX1wV6Xhaap"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}